---
title: "ames iowa kaggle home price modeling"
author: "preeti swaminathan & partick mcdevitt"
date: "14 avril 2017"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())

library(ggplot2)
library(MASS)
library(car)

home_dir <- "~/_smu/_src/home_prices/"
setwd(home_dir)
data_dir <- "./data"


```

## House Prices - à la Kaggle

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.  
With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.  
  
Deliverables:  
Your group is to turn in a paper that is no more that 7 pages long (without the appendix). Please put your code in the appendix, but any graphs and tables in the body of the paper.   
  
Sample Format  
Required deliverables in the complete report.  The format of your paper (headers, sections, etc) is flexible although should contain the following information:  
  
Introduction   
  
Data Description   
(Where did the data come from?  How big is it? How many observations?  Where can we find out more?  What are the specific variables that we need to know to understand with respect to your analysis?)  
  
Analysis Question 1:  
	Restatement of Problem   
  
	Specify the Model  
	   
	Checking Assumptions   
		Residual Plots   
		Influential point analysis (Cook’s D and Leverage)  
		Make sure and address each assumption.  
  
	Comparing Competing Models  
		adj R2    
		Interval CVPress    
	  
	Parameter Interpretation  
		Interpretation   
		Confidence Intervals   
  
Conclusion  
	A short summary of the analysis.  
	  
  
Analysis Question 2  
Restatement of Problem   

Estimating market value of a home for sale has significant implications for all parties involved in the transaction : seller, buyer, agents, mortgage providers and even local taxing authorities. Getting it right can improve local economies. Inefficiencies associated to historical methods of value assessments create hesitation on the part of buyers and lenders, and potential loss of revenue for sellers and agents. Developing a model that considers all available factors and provides a transparent valuation that can be shared among all parties in the transaction can enable the participants to proceed with increased confidence, thus increasing the velocity of the local real estate market.
That is the purpose of this evaluation : use all available contributing factors for the residnetial real estate market in Ames, Iowa and create a predicitve model to better estimate market valuation for future properties to be proposed for sale.

For this evaluation, there are seventy-nine explanatory variables available for exploitation, based on residential sales in the years 2006 trough 2010, comprising approximately 1500 sales. The explanatory variables include traditional expected characteristics, such as : neighborhood, square footage, number of bedrooms, number of bathrooms, etc. and also several factors that are perhaps considered secondary or tertiary, but are included in the modeling to increase predictive capability. Some of these additional factors include : heating type, number of fireplaces, qualitative assessment of the kitchen condition.

In order to build the model, the following steps are taken :
+ read in the raw training data set provided,  
+ basic cleaning of the data, including removing significant outliers (for this purpose, more than 5 std deviations from mean)  
+ imputing values for features where none was provided (for this purpose, setting to min value for numeric features, and creating a new factor level "None" for categorical features),  
+ plot and visually examine each feature in relation to log(SalePrice) ...  
++ this provides a basis for removing some features from consideration based on inspection  
++ some features may have 1400 / 1460 within same cateegory, thus not providing varaibility worthwhile including in a model  
++ some numerical features are sparesely populated, and the the few values visually exhibit zero slope in relation to log(SalePrice)  
++ a new feature was created "saledate" from the "year sold" and "month sold" features. Upon visual examination, there was no obvious trend in the time series view for  log(SalePrice)s, so this was eventually discarded (surprisingly, considering that the time period spanned the economic downturn to 2007 - 2009).   
++ this visual examination then results in eliminating approximately 25 of the features from consideration in the model.  
++ (All of the plots are available for review in the appendix, "homes_train_plots.pdf", if desired for review)

#### __Models Considered__  

In all cases, the basic data set consists of 52 predictor variables and the dependent output variable log(SalePrice)

Four different models are built :
			Stepwise  - using the R package stepAIC()  
			Forward   - using the R package stepAIC()  
			Backward  - using the R package stepAIC()  
			CUSTOM 	- this method using an iterative loop to evaluate the VIF for each contributor in the current model, eliminates the feature with the highest VIF, and continues until max(VIF) remaining in the model is < 5  
  
		Checking Assumptions   
			Residual Plots  
			- the following plots show the distribution of residuals and the predicted vs. the actual sale price (log scale) for the custom model. From these plots, we consider that the basic assumptions of normality in the residuals is achieved, and that the visual aspect of the fitted model aligns well with the dependent variable to be modeled.

			
![Residual Plots](custom_model.png)
	
			Influential point analysis (Cook’s D and Leverage)  
			- Cook's Distance is assessed for the fitted model : all values are significantly less than 1; with this large data set, there are no individual points which are signficantly affecting model resutls, either from perspective of leverage or influence based on Cook's distance for this model. Thus, model is apprpriate to maintain the complete modeled data set.

		Comparing Competing Models  
		
			Adj R2     
			- forward : 0.93
			- backward : 0.93
			- step : 0.93
			- custom : 0.90
			
			Kaggle Score   
			- forward : 0.38
			- backward : 0.38
			- step : 0.38
			- custom : 0.24
			
  
		Conclusion: 
		- For this effort, the 4 models each provide good predictive capability for estimating market valuation of residential to be proposed for offering in the Ames, Iowa market. The custom model outperformed the stepwise, forward addition and backward elimination methods for the features chosen in this case.
	  
  
```{r read data}

	setwd(data_dir)
	homes <- read.csv("train.csv", stringsAsFactors = FALSE)
	setwd(home_dir)

	names(homes) <- tolower(names(homes))
	
	for (i in 2:(length(homes)))
	{
		if (class(homes[,i]) == "character")
		{
			homes[,i] <- factor (homes[,i])
		}
	}
```

```{r remove outliers}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	remove outliers ... more than 5 sigma from mean value
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
	
	lst <- length(homes) - 1	# sale price is (currently) last column
	
	for (i in 2 : lst)
	{
		if(class(homes[,i]) == "integer" || class(homes[,i]) == "numeric")
		{
			homes[,i][which(scale(homes[,i]) > 5)] <- NA
			homes[,i][which(scale(homes[,i]) < -5)] <- NA
		}
	}

```

```{r new and scale}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	create a few new columns
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	dates <- paste(homes$yrsold, sprintf("%02d", homes$mosold), "01")
	homes$sale_date <- as.Date(dates, "%Y %m %d")
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	scale each column independently
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

#	for (i in 2 : length(homes))
#	{
#		if(class(homes[,i]) == "integer" || class(homes[,i]) == "numeric")
#		{
#			homes[,i] <- scale(homes[,i])
#		}
#	}

```

```{r plot base data}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	make some plots for numberic variables... linear, log_x, log_y, log_xy ...
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

#	pdf ("homes_train_plots.pdf", width = 10, height = 7)

	par (mfrow = c (2, 3))
	for (i in 2:(length(homes)))
	{
		if(class(homes[,i]) == "integer" || class(homes[,i]) == "numeric" || class(homes[,i]) == "matrix")
		{
			plot (homes[,i], main = (names(homes[i])))
			hist(homes[,i])
			plot(log(homes$saleprice)  ~ homes[,i])
		}
	}

	par (mfrow = c (2, 2))
	for (i in 2:(length(homes)))
	{
		if(class(homes[,i]) == "factor")
		{
			plot_title <- names(homes[i])

			p <- ggplot(homes, aes(x = homes[,i], fill = homes[,i])) + geom_bar() + labs(title = plot_title)
			print(p)

			p <- ggplot(homes, aes(x = homes[,i], y = log(saleprice), fill = homes[,i])) + geom_boxplot() + labs(title = plot_title)
			print(p)
		}
	}	
		
			plot(homes$saleprice ~ homes$sale_date)

#	dev.off()
```

```{r first basic fit} 

	for (i in 2:(length(homes)))
	{
		if(class(homes[,i]) == "integer" || class(homes[,i]) == "numeric" || class(homes[,i]) == "matrix")
		{
			fit <- lm(homes$saleprice ~ homes[,i])
			
			print(sprintf(" ... %3d : %20s | r^2 = %8.3f | p-value = %12.4e",
						  i, names(homes[i]), summary(fit)$r.squared, summary(fit)$coefficients[,4][2] ))
		}
	}

```

``` {r visual removals}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	Columns to remove - based on visual inspection
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
	
	homes_subset <- subset(homes, select = -c(
		id,
		mssubclass,
		street,
		alley,
		utilities,
		condition2,
		roofmatl,
		centralair,
		bsmtfinsf2,
		fence,
		miscfeature,
		lowqualfinsf,
		bsmthalfbath,
		kitchenabvgr,
		x3ssnporch,
		screenporch,
		garagequal,
		garagecond,
		paveddrive,
		poolarea,
		poolqc,
		miscval,
		mosold,
		yrsold,
		exterior2nd,
		bsmtcond,
		bsmtfintype1,
		garagefinish ))

```

```{r impute}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	Impute NAs to functional value
# ...
# ...	--> for numerical variables - impute to min value in
# ...	--> for factor variables - create new factor "None"
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	for (i in 1 : (length(homes_subset)))
	{
		if(class(homes_subset[,i]) == "integer" || class(homes_subset[,i]) == "numeric" || class(homes_subset[,i]) == "matrix")
		{
			homes_subset[,i][is.na (homes_subset[,i])] <- min (homes_subset[,i], na.rm = TRUE)
		}
	}
	
	for (i in 1 : (length(homes_subset)))
	{
		if(class(homes_subset[,i]) == "factor")
		{
			levels <- levels(homes_subset[,i])
			levels[length(levels) + 1] <- "None"
			homes_subset[,i] <- factor(homes_subset[,i], levels = levels)
			homes_subset[,i][is.na (homes_subset[,i])] <- "None"
		}
	}

```

```{r save subsetted data}

	homes_subset$log_saleprice <- log(homes_subset$saleprice)
	
	homes_subset <- subset(homes_subset, select = -c(saleprice, sale_date))
	
	sas_dir <- "~/sas/SASUniversityEdition/myfolders/"
	setwd(sas_dir)
	write.csv (homes_subset, file = "training_set_cleaned.csv", row.names = FALSE)
	setwd(home_dir)

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	save base data set for multiple solution options
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	homes_subset_base <- homes_subset

```


```{r custom model}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	base model on all candidate features
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	homes_subset <- homes_subset_base

	df_fits <- data.frame(row = integer(0), feature = character(0), r_squared = numeric(0), stringsAsFactors = FALSE)
	
	for (i in 1 : (length(homes_subset)))
	{
			fit <- lm(homes_subset$log_saleprice ~ homes[,i])
			
			print(sprintf(" ... %3d : %20s | r^2 = %8.3f | p-value = %12.4e",
						  i, names(homes_subset[i]), summary(fit)$r.squared,
						  		summary(fit)$coefficients[,4][2] ))
			
			next_row <- data.frame(row = i, feature = names(homes_subset[i]),
								   r_squared = summary(fit)$r.squared)
			df_fits <- rbind(df_fits, next_row)
	}

	df_sort <- df_fits[with(df_fits, order(-r_squared)), ]
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	do some feature reduction based on VIF
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
	
	homes_subset <- homes_subset_base
	
	fit_all <- lm (log_saleprice ~ ., data = homes_subset)
	
	vif_lst <- vif(fit_all)
	vif_lst[order(vif_lst)]

	iter <- 1
	
	while ( vif_lst[order(-vif_lst[,1]),][1] > 5)
	{
		rmv_col <- order(-vif_lst[,1])[1]

				prnt_line <- paste(iter, "remove column :",
					   names(homes_subset)[[rmv_col]],
						"VIF = ", round(vif_lst[order(-vif_lst[,1]),][1], 2))

		print (prnt_line)
   
		homes_subset[, rmv_col] = NULL
		
		fit_all <- lm (log_saleprice ~ .,
						data = homes_subset)
		
		summary(fit_all)
		
		vif_lst <- vif(fit_all)
		vif_lst[order(vif_lst)]
	
		iter <- iter + 1
	}
	
	cat ("\n-=-=-=-=-=   Colinearity reduced   -=-=-=-=-=-=\n")
	
	names (homes_subset)
	
	fit_all <- lm (log_saleprice ~ .,
						data = homes_subset)
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	plot some residuals and influence diagnostics plots
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	png(filename = "custom_model.png", width = 2100, height = 700, units = "px")
	
	par (mfrow = c (2, 2))
	
	resid <- predict(fit_all) - homes_subset$log_saleprice
	hist(resid)	
	plot(predict(fit_all) - homes_subset$log_saleprice, main = "Residuals by Observation")
	plot(predict(fit_all) ~ homes_subset$log_saleprice, main = "Predicted log(Sale Price) vs. Train Set log(Sale Price)")

# Influential Observations - Cook's D plot

	cooks_d = cooks.distance(fit_all)
	plot(cooks_d, ylab = "Cooks distances", main = "Cook_s Distances")

	dev.off()	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	test cases
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
	
	setwd(data_dir)
	test_homes <- read.csv("test.csv", stringsAsFactors = FALSE)
	setwd(home_dir)
	
	names(test_homes) <- tolower(names(test_homes))
	
	for (i in 2:(length(test_homes)))
	{
		if (class(test_homes[,i]) == "character")
		{
			test_homes[,i] <- factor (test_homes[,i])
		}
	}

	plot(predict(fit_all, newdata = test_homes))
	
	test_homes$pred_log_saleprice <- predict(fit_all, newdata = test_homes)
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	submittal file
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	df_submit <- data.frame(test_homes$id, exp(test_homes$pred_log_saleprice))
	names(df_submit) <- c("Id", "SalePrice")
	
	df_submit$SalePrice[is.na(df_submit$SalePrice)] <- median(df_submit$SalePrice, na.rm = TRUE)

	write.csv(df_submit, file = "submit_test_pred_custom.csv", row.names = FALSE)
	
```

```{r forward}
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
#  ...	Stepwise Regression - Forward
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

library (MASS)
	
	homes_subset <- homes_subset_base

	fit_forward <- lm (log_saleprice ~ ., data = homes_subset)
	
	AIC(fit_forward)
	
	step_fwd <- stepAIC(fit_forward, direction = "forward")
	
	step_fwd$anova # display results 
	
	plot(predict(step_fwd, newdata = test_homes))
	
	test_homes$pred_fwd_log_saleprice <- predict(step_fwd, newdata = test_homes)
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	submittal file - forward selection
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	df_submit_fwd <- data.frame(test_homes$id, exp(test_homes$pred_fwd_log_saleprice))
	names(df_submit_fwd) <- c("Id", "SalePrice")
	
	df_submit_fwd$SalePrice[is.na(df_submit_fwd$SalePrice)] <- median(df_submit_fwd$SalePrice, na.rm = TRUE)

	write.csv(df_submit_fwd, file = "submit_test_pred_fwd_selection.csv", row.names = FALSE)

```

```{r backward}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
#  ...	Stepwise Regression - Backward
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
	
	homes_subset <- homes_subset_base

	fit_backward <- lm (log_saleprice ~ ., data = homes_subset)
	step <- stepAIC(fit_backward, direction = "backward")
	step$anova # display results 
	
	plot(predict(fit_backward, newdata = test_homes))
	
	test_homes$pred_bwd_log_saleprice <- predict(fit_backward, newdata = test_homes)
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	submittal file - backward selection
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	df_submit_bwd <- data.frame(test_homes$id, exp(test_homes$pred_bwd_log_saleprice))
	names(df_submit_bwd) <- c("Id", "SalePrice")
	
	df_submit_bwd$SalePrice[is.na(df_submit_bwd$SalePrice)] <- median(df_submit_bwd$SalePrice, na.rm = TRUE)

	write.csv(df_submit_bwd, file = "submit_test_pred_bwd_selection.csv", row.names = FALSE)

```

```{r stepwise}

# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
#  ...	Stepwise Regression - both
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

library (MASS)
	
	homes_subset <- homes_subset_base

	fit_both <- lm (log_saleprice ~ ., data = homes_subset)
	step <- stepAIC(fit_both, direction = "both")
	step$anova # display results 
	
	plot(predict(fit_both, newdata = test_homes))
	
	test_homes$pred_both_log_saleprice <- predict(fit_both, newdata = test_homes)
	
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ...	submittal file - both selection
# ...	-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

	df_submit_both <- data.frame(test_homes$id, exp(test_homes$pred_both_log_saleprice))
	names(df_submit_both) <- c("Id", "SalePrice")
	
	df_submit_both$SalePrice[is.na(df_submit_both$SalePrice)] <- median(df_submit_both$SalePrice, na.rm = TRUE)

	write.csv(df_submit_both, file = "submit_test_pred_both_selection.csv", row.names = FALSE)

```
